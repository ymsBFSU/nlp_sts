{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Jim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "# needs to be installed first\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk import download\n",
    "from nltk.corpus import treebank\n",
    "download('treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "train_data = treebank.tagged_sents()\n",
    "\n",
    "# Perceptron tagger\n",
    "per = PerceptronTagger(load='false')\n",
    "per.train(train_data)\n",
    "\n",
    "file_prefix = lambda stage: stage + '/STS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input data (train/test set + golden standards) and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(filenames, file_prefix, **removals):\n",
    "    \"\"\"\n",
    "    Reads data and corresponding golden standard file as the true label\n",
    "    \"\"\"\n",
    "    input_prefix = file_prefix + '.input.'\n",
    "    gs_prefix = file_prefix + '.gs.'\n",
    "    input = pd.DataFrame()\n",
    "    for filename in filenames:\n",
    "        sentences = pd.read_csv(input_prefix + filename + '.txt',\n",
    "                                sep='\\t', names=['sentence1', 'sentence2'],\n",
    "                                quoting=3)\n",
    "        golden_standards = pd.read_csv(gs_prefix + filename + '.txt',\n",
    "                                       names=['golden_standard'])\n",
    "        dfX = pd.concat([sentences, golden_standards], axis=1)\n",
    "        input = pd.concat([input, dfX])\n",
    "\n",
    "    input.reset_index(drop=True)\n",
    "    # Remove punctuation (and numbers)\n",
    "    input[['sentence1', 'sentence2']] = input[['sentence1', 'sentence2']].apply(lambda col: sentClean(col, **removals))\n",
    "    # Tokenize (remove stop words)\n",
    "    input[['sentence1', 'sentence2']] = input[['sentence1', 'sentence2']].apply(lambda col: sentTokenize(col, **removals))\n",
    "    return input\n",
    "\n",
    "def sentClean(sents, **removals):\n",
    "    \"\"\"\n",
    "    Clean data from punctuations and numbers\n",
    "    \"\"\"\n",
    "    new_sent = []\n",
    "    for sent in sents:\n",
    "        if removals.get('numbers'):\n",
    "            mod_sent = re.sub(r'[^A-Z a-z]', ' ', sent)\n",
    "        else:\n",
    "            mod_sent = re.sub(r'[^\\w]', ' ', sent)  # |\\b\\w\\b -> to take out single chars\n",
    "        clean_sent = re.sub(r'[ ]+', ' ', mod_sent.strip())\n",
    "        if len(clean_sent) == 0:\n",
    "            clean_sent = sent\n",
    "        new_sent.append(clean_sent)\n",
    "    return new_sent\n",
    "\n",
    "def sentTokenize(sents, **removals):\n",
    "    \"\"\"\n",
    "    Tokenize sentences and remove stop words if requested\n",
    "    \"\"\"\n",
    "    new_sent = []\n",
    "    for sent in sents:\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        if removals.get('stop_words'):\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "        new_sent.append(tokens)\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for transforming sentences and finding their similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" more orthodox and robust implementation \"\"\"\n",
    "def dice_coefficient(s1, s2, n=2):\n",
    "    \"\"\"dice coefficient 2nt/na + nb.\"\"\"\n",
    "    if isinstance((s1), list):\n",
    "        a = ' '.join(s1)\n",
    "    elif isinstance(s1, str):\n",
    "        a = s1\n",
    "    if isinstance((s2), list):\n",
    "        b = ' '.join(s2)\n",
    "    elif isinstance(s2, str):\n",
    "        b = s2\n",
    "    if not len(a) or not len(b): return 0.0\n",
    "    if len(a) == 1:  a = a + u'.'\n",
    "    if len(b) == 1:  b = b + u'.'\n",
    "\n",
    "    a_bigram_list = []\n",
    "    for i in range(len(a) - 1):\n",
    "        a_bigram_list.append(a[i:i + n])\n",
    "    b_bigram_list = []\n",
    "    for i in range(len(b) - 1):\n",
    "        b_bigram_list.append(b[i:i + n])\n",
    "\n",
    "    a_bigrams = set(a_bigram_list)\n",
    "    b_bigrams = set(b_bigram_list)\n",
    "    overlap = len(a_bigrams & b_bigrams)\n",
    "    dice_coeff = overlap * 2.0 / (len(a_bigrams) + len(b_bigrams))\n",
    "    return dice_coeff\n",
    "\n",
    "def myWSD(pt_pair, context):\n",
    "    '''\n",
    "    Returns the synset if it's a noun, a verb, an adverb\n",
    "    or an adjective or the lowered word\n",
    "    '''\n",
    "    wn_postag_map = {'N': 'n',\n",
    "             'V': 'v',\n",
    "             'J': 'a',\n",
    "             'R': 'r'\n",
    "            }\n",
    "\n",
    "    pt = wn_postag_map.get(pt_pair[1][0])\n",
    "    word = pt_pair[0]\n",
    "    if pt:\n",
    "        return mylesk(context, word, pt)\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical and semantic transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(p):\n",
    "    if p[1][0] in {'N','V'}:\n",
    "        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
    "    return p[0].lower()\n",
    "\n",
    "def lemmas(sents):\n",
    "    new_sent = []\n",
    "    for sent in sents:\n",
    "        pairs = per.tag(sent)\n",
    "        new_sent.append([lemmatize(pair) for pair in pairs])\n",
    "    return new_sent\n",
    "\n",
    "def ngrams(sents, n, word=True):\n",
    "    \"\"\"\n",
    "    Applying ngrams with default behaviour to apply it on words\n",
    "    \"\"\"\n",
    "    new_sent = []\n",
    "    n_old = n\n",
    "    for sent in sents:\n",
    "        n = n_old\n",
    "        # Handle sentences smaller than n\n",
    "        if len(sent) < n:\n",
    "            n_old = n\n",
    "            n = len(sent)\n",
    "        if word:\n",
    "            grams_lst = [w for w in nltk.ngrams(sent, n)]\n",
    "            new_sent.append(grams_lst)\n",
    "        else:\n",
    "            sent_joined = ' '.join(sent)\n",
    "            grams_lst = [''.join(ch) for ch in nltk.ngrams(sent_joined, n)]\n",
    "            new_sent.append(grams_lst)\n",
    "    return new_sent\n",
    "\n",
    "def mylesk(context_sentence, ambiguous_word, pos=None, synsets=None):\n",
    "    \"\"\"\n",
    "    Optimization of lesk for word disambiguation which uses the definitions\n",
    "    and all the examples of all the synset and all of its hypernyms. It returns\n",
    "    back the best matched hypernym.\n",
    "    \"\"\"\n",
    "    context = set(context_sentence)\n",
    "    hyper = lambda s: s.hypernyms()\n",
    "    if synsets is None:\n",
    "        synsets = wn.synsets(ambiguous_word)\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return ambiguous_word\n",
    "\n",
    "    max_sense = []\n",
    "    for ss in synsets:\n",
    "        ss_lst = list(ss.closure(hyper))\n",
    "        ss_lst.append(ss)\n",
    "        for ss_hyper in ss_lst:\n",
    "            max_sense.append(\n",
    "                (len(context.intersection(ss_hyper.definition().split())), ss_hyper, ss)\n",
    "                # (self.dice_coefficient(context, ss_hyper.definition()), ss_hyper, ss)\n",
    "            )\n",
    "            for ex in ss_hyper.examples():\n",
    "                if ex:\n",
    "                    max_sense.append(\n",
    "                        (len(context.intersection(ex.split())), ss_hyper, ss)\n",
    "                        # (self.dice_coefficient(context, ex), ss_hyper, ss)\n",
    "                    )\n",
    "    _, hyper, sense = max(max_sense)\n",
    "    return hyper.lemmas()[0].name()\n",
    "\n",
    "def lesk(sents):\n",
    "    new_sent = []\n",
    "    for sent in sents:\n",
    "        pairs = per.tag(sent)\n",
    "        new_sent.append([myWSD(pair,sent) for pair in pairs])\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model1 & Model2 construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFeatures(df_sents, files='', stage=''):\n",
    "    \"\"\"\n",
    "    Constructing the features of the Model1 using lexical and semantic transformations\n",
    "    \"\"\"\n",
    "    Xlesk = df_sents.apply(lesk)\n",
    "\n",
    "    Mdice_lem = df_sents.apply(lambda x: dice_coefficient(x['sentence1'], x['sentence2']), axis=1)\n",
    "\n",
    "    Mjac_lem = df_sents.apply(lambda x: jaccard_distance(set(x['sentence1']), set(x['sentence2'])), axis=1)\n",
    "\n",
    "    Mlesk = Xlesk.apply(lambda x: dice_coefficient(x['sentence1'], x['sentence2']), axis=1)\n",
    "\n",
    "    Xngram = df_sents.apply(lambda col: ngrams(col, 2))\n",
    "    Mngram2 = Xngram.apply(lambda x: jaccard_distance(set(x['sentence1']), set(x['sentence2'])), axis=1)\n",
    "\n",
    "    Xngram = df_sents.apply(lambda col: ngrams(col, 4))\n",
    "    Mngram4 = Xngram.apply(lambda x: jaccard_distance(set(x['sentence1']), set(x['sentence2'])), axis=1)\n",
    "\n",
    "    # Use of features without stop words\n",
    "    removals = {\n",
    "        'stop_words': True,\n",
    "        'numbers': True\n",
    "    }\n",
    "    df_XtrainSW = getData(files, file_prefix(stage), **removals)\n",
    "\n",
    "    df_XtrainSW = df_XtrainSW.drop(df_XtrainSW.columns[len(df_XtrainSW.columns) - 1], axis=1)\n",
    "\n",
    "    Xlem = df_XtrainSW.apply(lemmas)\n",
    "\n",
    "    Xngram = Xlem.apply(lambda col: ngrams(col, 1))\n",
    "    Mngram1_sw = Xngram.apply(lambda x: jaccard_distance(set(x['sentence1']), set(x['sentence2'])), axis=1)\n",
    "\n",
    "    Xngram = Xlem.apply(lambda col: ngrams(col, 3))\n",
    "    Mngram2_sw = Xngram.apply(lambda x: jaccard_distance(set(x['sentence1']), set(x['sentence2'])), axis=1)\n",
    "    \n",
    "    df = pd.concat([Mdice_lem, Mjac_lem, Mlesk, Mngram2, Mngram4, Mngram1_sw, Mngram2_sw], axis=1)\n",
    "    return df\n",
    "\n",
    "def fit_model2(X):\n",
    "    \"\"\"\n",
    "    The train model for the Model2 using Tf/Idf over bag of words \n",
    "    \"\"\"\n",
    "    bow = CountVectorizer(lowercase=False,\n",
    "                          analyzer=lambda x: x)\n",
    "    join_X = X['sentence1'] + X['sentence2']\n",
    "    bow_Xtrn = bow.fit_transform(join_X)\n",
    "    sents_tfidf = TfidfTransformer()\n",
    "    tfidf_Xtrn = sents_tfidf.fit_transform(bow_Xtrn)\n",
    "    return bow, sents_tfidf, tfidf_Xtrn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model1 Pearson Correlation: 0.6318505133806376\n",
      "Duration: 122.60720443725586\n",
      "\n",
      "Model2 Pearson Correlation: 0.6255477067661559\n",
      "Duration: 30.31155824661255\n",
      "\n",
      "Final Combined Model Pearson Correlation: 0.7519449894980038\n",
      "Global duration: 186.37055492401123\n"
     ]
    }
   ],
   "source": [
    "filenames_train = ['MSRpar', 'MSRvid', 'SMTeuroparl']\n",
    "filenames_test = ['MSRpar', 'MSRvid', 'SMTeuroparl',\n",
    "                  'surprise.OnWN', 'surprise.SMTnews']\n",
    "\n",
    "removals = {\n",
    "            'stop_words': False,\n",
    "            'numbers': True\n",
    "            }\n",
    "\n",
    "start_global = time()\n",
    "# Tokenized raw train data\n",
    "df_Xtrain = getData(filenames_train, file_prefix('train'), **removals)\n",
    "labels_trn = df_Xtrain.iloc[:, -1]\n",
    "df_Xtrain = df_Xtrain.drop(df_Xtrain.columns[len(df_Xtrain.columns) - 1], axis=1)\n",
    "\n",
    "# Tokenized raw test data\n",
    "df_Xtest = getData(filenames_test, file_prefix('test'), **removals)\n",
    "labels_tst = df_Xtest.iloc[:, -1]\n",
    "df_Xtest = df_Xtest.drop(df_Xtest.columns[len(df_Xtest.columns) - 1], axis=1)\n",
    "\n",
    "# Base transformations of sentences\n",
    "Xlem = df_Xtrain.apply(lemmas)\n",
    "Xlem_tst = df_Xtest.apply(lemmas)\n",
    "\n",
    "## Model1\n",
    "start_model = time()\n",
    "# Transform the train data\n",
    "transformed_train = modelFeatures(Xlem, filenames_train, 'train')\n",
    "\n",
    "# Model1 training algorithm\n",
    "reg_model = AdaBoostRegressor()\n",
    "# Construct feature of Model1 for ensemble model\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "results_model1 = model_selection.cross_val_predict(reg_model, transformed_train, labels_trn, cv=kfold)\n",
    "\n",
    "# Transformation of test data\n",
    "transformed_test = modelFeatures(Xlem_tst, filenames_test, 'test')\n",
    "\n",
    "# Make predictions over test data for Model1 (output of Model1)\n",
    "pred_model = reg_model.fit(transformed_train, labels_trn).predict(transformed_test)\n",
    "print()\n",
    "print('Model1 Pearson Correlation:', pearsonr(pred_model.T.tolist(), labels_tst.tolist())[0])\n",
    "duration_model = time() - start_model\n",
    "print('Duration:', duration_model)\n",
    "\n",
    "## Model2\n",
    "start_model = time()\n",
    "# Transform data \n",
    "bow, sents_tfidf, tfidf_Xtrn = fit_model2(Xlem)\n",
    "# Model2 training algorithm\n",
    "reg_bow = GradientBoostingRegressor()\n",
    "\n",
    "# Construct feature of Model2 for ensemble model (output of Model2)\n",
    "results_model2 = model_selection.cross_val_predict(reg_bow, tfidf_Xtrn, labels_trn, cv=kfold)\n",
    "# join test sentences for applying bag of words with TF/IDF\n",
    "join_Xlem_tst = Xlem_tst['sentence1'] + Xlem_tst['sentence2']\n",
    "bow_Xtst = bow.transform(join_Xlem_tst)\n",
    "tfidf_Xtst = sents_tfidf.transform(bow_Xtst)\n",
    "\n",
    "# Make predictions over test data for Model2\n",
    "pred_bow = reg_bow.fit(tfidf_Xtrn, labels_trn).predict(tfidf_Xtst)\n",
    "print()\n",
    "print('Model2 Pearson Correlation:', pearsonr(pred_bow.T.tolist(), labels_tst.tolist())[0])\n",
    "duration_model = time() - start_model\n",
    "print('Duration:', duration_model)\n",
    "\n",
    "## Ensemble Model3\n",
    "reg_final = XGBRegressor().fit(np.concatenate([results_model1.reshape(-1,1), results_model2.reshape(-1,1)], axis=1), labels_trn)\n",
    "pred_final = reg_final.predict(np.concatenate([pred_model.reshape(-1,1), pred_bow.reshape(-1,1)], axis=1))\n",
    "print()\n",
    "print('Final Combined Model Pearson Correlation:', pearsonr(pred_final.T.tolist(), labels_tst.tolist())[0])\n",
    "duration = time() - start_global\n",
    "print('Global duration:', duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The final correlation between the golden standard of the test sentences and our prediction model is 0.752. Overall, this shows that our model can find some textual similarities between sentences and give results quite close to the real ones.\n",
    "\n",
    "We noticed through the pre-work that some simple features were giving good results like lemmas even when we were not adding more information (features). Synsets on the other hand were not giving good results alone, that's why we used them through our own optimization of lesk algorithm for disambiguating words. Furthermore, specific sizes of ngrams were proved to be quite informative as features of our model. Finally, the use of Tf/Idf metric over bag of words gave some unexpectedly good results because of similarities in the train and test set topics. We decided directly to keep this approach as another model because of the size of its feature space. Also, we didn't want to put in the same feature matrix so different kind of metrics.\n",
    "\n",
    "At the end, we ended up using the following transformations on the data (sentences) before applying a similarity metric:\n",
    "* lemmas (as a base transformation for the rest)\n",
    "* n-grams\n",
    "* word substitutions using word disambiguation with our optimization of lesk.\n",
    "* bag of words\n",
    "\n",
    "At this point, we need to specify that in order to get a better representation of a sentence we substituted words with the (first) lemma of the best matched hypernym synset. We thought that the hypernym between 2 words of different senteces can be the same more frequently than the synset of the word we wanted to disabiguate. Also, in order to avoid errors we return the same word in case there is no synset for a word.\n",
    "\n",
    "The similarity metrics we used were:\n",
    "* Dice coefficient\n",
    "* Jaccard distance\n",
    "* Tf/Idf (it's more like a statistic \n",
    "\n",
    "Dice coefficient values more the intersection of the sentences and uses bi-grams. It proved to give consistently better results, whenever we managed to use it, in comparison with jaccard distance.\n",
    "\n",
    "Finally, we constructed 2 different models:\n",
    "* Model1, which includes the following features:\n",
    "    * Dice coefficient over lemmas\n",
    "    * Dice coefficient over Lesk optimization\n",
    "    * Jaccard distance over lemmas\n",
    "    * Jaccard distance over lemmas without stopwords\n",
    "    * Jaccard distance over lemmas of n-grams:\n",
    "        * 2 words\n",
    "        * 4 words\n",
    "        * 3 words without stopwords\n",
    "    \n",
    "* Model2, which is the Tf/Idf statistic over bag of words.\n",
    "\n",
    "Our problem is a regression problem. We tested different regressors and at the end we ended up using AdaBoostRegressor for Model1 and GradientBoostingRegressor for Model2.\n",
    "\n",
    "In order to combine the models we applied an ensemble technique called stacking using K-folds, K=10, splitting our training set into 9 folds for training and 1 for prediction. In every fold we get a prediction over the prediction set; then we construct a vector with all the predictions which represents the testing error over the training folds. After doing this for both models, we train with the whole training dataset for getting the predictions/output of each model over the test set.\n",
    "\n",
    "Then, after constructing a vector from the K-fold of each model, we use them as features for our ensemble model which will combine the results of the other 2 models we described. Like this, it would be like training over the (simulated) testing error. That technique boosted our model and made it much stronger since it combines the information from both and makes our final model learn some inaccuracies of the other 2 base models. For that model we used Extreme Gradient Boosting Regressor, which is a very famous algorithm used in most of AI challenges.\n",
    "\n",
    "In general, we noticed that simple features we had tried during the course were giving better results than more complex ones; that's why at the end, our models don't use any kind of special features. Also, we took advantage of the fact that the source of the training and some of the test (3 out of 5) sets is the same and we used bag of words with Tf/Idf. Finally, the addition of similar transformations using the same similarity metric gave a rise to our final results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
